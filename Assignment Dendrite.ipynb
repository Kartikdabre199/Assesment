{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e5c713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72e0c43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path_data,path_json):\n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    #Reading the data from csv file\n",
    "\n",
    "    data=pd.read_csv(path_data)\n",
    "    print(data.head(5))\n",
    "    print(data.columns)\n",
    "\n",
    "    #Reading the string from the rtf file\n",
    "    from striprtf.striprtf import rtf_to_text\n",
    "    file_cont=open(path_json,'r')\n",
    "    rtf=file_cont.read()\n",
    "    file_cont.close()\n",
    "    text = rtf_to_text(rtf)\n",
    "    #print(text)\n",
    "    dict={text}\n",
    "\n",
    "    #Parsing the string using json format to convert to a python object\n",
    "    import json\n",
    "    p_ob=json.loads(text)\n",
    "    target=p_ob['design_state_data']['target']\n",
    "    feature_handling=p_ob['design_state_data']['feature_handling']\n",
    "    print(feature_handling.keys())\n",
    "\n",
    "    #imputing missing values\n",
    "    return(data,p_ob,target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943ff98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_num(val,data,col):\n",
    "    import numpy as np\n",
    "\n",
    "    #print(\"here\")\n",
    "    if 'average' in val.lower():\n",
    "        #print(\"there\")\n",
    "        data.loc[data.loc[:,col].isna(),col]=np.mean(data.loc[:,col])\n",
    "\n",
    "    if 'custom' in val.lower():\n",
    "            #print(\"where\")\n",
    "            data.loc[data.loc[:, col].isna(), col] = np.median(data.loc[:, col])\n",
    "    return (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c62e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_cat(val,data,col):\n",
    "    new_df=pd.DataFrame()\n",
    "    if \"hash\" in method.lower():\n",
    "        obj=HashingVectorizer(n_features=5)\n",
    "        new_df=obj.fit_transform(data.loc[:,col])\n",
    "    return(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f74828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data,target):\n",
    "\n",
    "    cols=data.columns\n",
    "\n",
    "    target=target['target']\n",
    "    y=(data.loc[:,target])\n",
    "    X=(data.loc[:,cols!=target])\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=2)\n",
    "    return(X_train,X_test,y_train,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "084194f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_reduction(method,data_X,data_y,details):\n",
    "    if 'tree' in method.lower():\n",
    "        n_trees=int(details['num_of_trees'])\n",
    "        d_trees=int(details['depth_of_trees'])\n",
    "        f_to_keep=int(details['num_of_features_to_keep'])\n",
    "        print(f_to_keep)\n",
    "        features=data_X.columns\n",
    "        #Random forest or ensemble of decision trees for feature_reduction\n",
    "        estimator=RandomForestRegressor(n_estimators=n_trees,random_state=9)\n",
    "        estimator.fit(data_X,data_y)\n",
    "        f_i = list(zip(features,estimator.feature_importances_))\n",
    "        sel_f=RFE(estimator,n_features_to_select=f_to_keep)\n",
    "        sel_f.fit(data_X,data_y)\n",
    "        selected_features=np.array(features)[sel_f.get_support()]\n",
    "        return(data_X.loc[:,selected_features])\n",
    "\n",
    "    if \"no reduction\" in method.lower():\n",
    "        return (data_X)\n",
    "    if \"pca\" in method.lower():\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(data_X)\n",
    "        data_X=scaler.transform(data_X)\n",
    "        pca=PCA(.95)\n",
    "        pca.fit(data_X)\n",
    "        data_X=pca.transform(data_X)\n",
    "        return (data_X)\n",
    "    if 'corr' in method.lower():\n",
    "        cor_mat=data_X.corr().abs()\n",
    "        print(cor_mat)\n",
    "        upper_tri=cor_mat.where(np.triu(np.ones(cor_mat.shape),k=1).astype(np.bool))\n",
    "        print(upper_tri)\n",
    "        to_drop=[col for col in upper_tri.columns if any(upper_tri[col]>0.95)]\n",
    "        print(to_drop)\n",
    "        data_X=data_X.drop(data_X.columns[to_drop],axis=1)\n",
    "        return (data_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36ffc031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_build(algo,X_train,y_train,p_ob):\n",
    "    if 'randomforestregressor' in algo.lower():\n",
    "        print(p_ob[algo].keys())\n",
    "        strat=p_ob[algo][\"feature_sampling_statergy\"]\n",
    "        #print(strat)\n",
    "        min_trees=int(p_ob[algo]['min_trees'])\n",
    "        max_trees=int(p_ob[algo]['max_trees'])\n",
    "        mini_depth=int(p_ob[algo]['min_depth'])\n",
    "        max_depth=int(p_ob[algo]['max_depth'])\n",
    "        min_sa_per_leaf=int(p_ob[algo]['min_samples_per_leaf_min_value'])\n",
    "        max_sa_per_leaf=int(p_ob[algo]['min_samples_per_leaf_max_value'])\n",
    "        if p_ob[algo]['parallelism']==0:\n",
    "            para=[False]\n",
    "        else:\n",
    "            para=[True]\n",
    "        param_grid = {'n_estimators': range(min_trees,max_trees+1),\n",
    "                       'max_depth': range(mini_depth, max_depth + 1),\n",
    "                       'min_samples_leaf': range(min_sa_per_leaf, max_sa_per_leaf + 1),\n",
    "                       'bootstrap': para}\n",
    "        rf = RandomForestRegressor()\n",
    "        grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
    "                                   cv=3, n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        print(grid_search.best_params_)\n",
    "        best_grid = grid_search.best_estimator_\n",
    "        predi=best_grid.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, predi)\n",
    "        rmse = np.sqrt(mse)\n",
    "        return (rmse)\n",
    "    if 'decisiontreeregressor' in algo.lower():\n",
    "        print(p_ob[algo].keys())\n",
    "        #strat=p_ob[algo][\"feature_sampling_statergy\"]\n",
    "        #print(strat)\n",
    "        splitt=[]\n",
    "        if p_ob[algo]['use_best']==True:\n",
    "            splitt=['best']\n",
    "        if p_ob[algo]['use_random']:\n",
    "            splitt=['random']\n",
    "\n",
    "\n",
    "        mini_depth=int(p_ob[algo]['min_depth'])\n",
    "        max_depth=int(p_ob[algo]['max_depth'])\n",
    "        min_sa_per_leaf=(p_ob[algo]['min_samples_per_leaf'])\n",
    "        #max_sa_per_leaf=int(p_ob[algo]['min_samples_per_leaf_max_value'])\n",
    "\n",
    "        param_grid = {'splitter':splitt,\n",
    "                       'max_depth': range(mini_depth, max_depth + 1),\n",
    "                       'min_samples_leaf': min_sa_per_leaf,\n",
    "                       }\n",
    "        dt = DecisionTreeRegressor()\n",
    "        grid_search = GridSearchCV(estimator=dt, param_grid=param_grid,\n",
    "                                   cv=3, n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        print(grid_search.best_params_)\n",
    "        best_grid = grid_search.best_estimator_\n",
    "        predi=best_grid.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, predi)\n",
    "        rmse = np.sqrt(mse)\n",
    "        return (rmse)\n",
    "\n",
    "\n",
    "\n",
    "    if 'gbtregressor' in algo.lower():#sub_Sample values should be in the range (0.0,1.0]\n",
    "        print(p_ob[algo].keys())\n",
    "        strat = p_ob[algo][\"feature_sampling_statergy\"]\n",
    "        #print(strat)\n",
    "        max_fea=[]\n",
    "        min_sub = (p_ob[algo]['min_subsample'])\n",
    "        max_sub = int(p_ob[algo]['max_subsample'])\n",
    "        mini_step = int(p_ob[algo]['min_stepsize'])\n",
    "        max_step = int(p_ob[algo]['max_stepsize'])\n",
    "        mini_depth = int(p_ob[algo]['min_depth'])\n",
    "        max_depth = int(p_ob[algo]['max_depth'])\n",
    "        min_iter = int(p_ob[algo]['min_iter'])\n",
    "        max_iter = int(p_ob[algo]['max_iter'])\n",
    "        n_est=(p_ob[algo]['num_of_BoostingStages'])\n",
    "        #cv=int(p_ob[algo]['num_of_BoostingStages'])\n",
    "        if 'fixed' in p_ob[algo]['feature_sampling_statergy'].lower():\n",
    "            max_fea=[int(p_ob[algo]['fixed_number'])]\n",
    "        param_grid={'n_estimators': n_est,\n",
    "                           'max_depth': range(mini_depth, max_depth + 1),\n",
    "\n",
    "                            #'subsample': range(min_sub,max_sub+1),\n",
    "                            'max_features':max_fea}\n",
    "        tree_regressor=GradientBoostingRegressor()\n",
    "        grid_search=GridSearchCV(estimator=tree_regressor, param_grid=param_grid,\n",
    "                                       cv=3, n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_grid = grid_search.best_estimator_\n",
    "        predi = best_grid.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, predi)\n",
    "        rmse = np.sqrt(mse)\n",
    "        return (rmse)\n",
    "    if 'elasticnetregression' in algo.lower():\n",
    "        print(p_ob[algo].keys())\n",
    "        maxi_iter=int(p_ob[algo]['max_iter'])\n",
    "        min_iter=int(p_ob[algo]['min_iter'])\n",
    "\n",
    "        min_alpha= float(p_ob[algo]['min_regparam'])\n",
    "        max_alpha=float(p_ob[algo]['max_regparam'])\n",
    "        min_en=float(p_ob[algo]['min_elasticnet'])\n",
    "        max_en=float(p_ob[algo]['max_elasticnet'])\n",
    "        param_grid = {'max_iter': range(min_iter,maxi_iter),\n",
    "                      'l1_ratio': [min_en, max_en],\n",
    "\n",
    "                      'alpha': [min_alpha, max_alpha]\n",
    "\n",
    "                      }\n",
    "        e_n = ElasticNet()\n",
    "        grid_search = GridSearchCV(estimator=e_n, param_grid=param_grid,\n",
    "                                   cv=3, n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        print(grid_search.best_params_)\n",
    "        best_grid = grid_search.best_estimator_\n",
    "        predi = best_grid.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, predi)\n",
    "        rmse = np.sqrt(mse)\n",
    "        return (rmse)\n",
    "    if 'lasso' in algo.lower():\n",
    "        print(p_ob[algo].keys()) #similar to other model building and evaluating afterwards.\n",
    "        lr=Lasso()\n",
    "        maxi_iter = int(p_ob[algo]['max_iter'])\n",
    "        min_iter = int(p_ob[algo]['min_iter'])\n",
    "\n",
    "        min_alpha = float(p_ob[algo]['min_regparam'])\n",
    "        max_alpha = float(p_ob[algo]['max_regparam'])\n",
    "\n",
    "        param_grid = param_grid = {'max_iter': range(min_iter,maxi_iter),\n",
    "\n",
    "\n",
    "                      'alpha': [min_alpha, max_alpha]\n",
    "\n",
    "                      }\n",
    "        grid_search=GridSearchCV(estimator=lr, param_grid=param_grid,\n",
    "                                   cv=3, n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        print(grid_search.best_params_)\n",
    "        best_grid = grid_search.best_estimator_\n",
    "        predi = best_grid.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, predi)\n",
    "        rmse = np.sqrt(mse)\n",
    "        return (rmse)\n",
    "    if 'ridge' in algo.lower():\n",
    "        print(p_ob[algo].keys())  # similar to other model building and evaluating afterwards.\n",
    "        rr = Ridge()\n",
    "        maxi_iter = int(p_ob[algo]['max_iter'])\n",
    "        min_iter = int(p_ob[algo]['min_iter'])\n",
    "\n",
    "        min_alpha = float(p_ob[algo]['min_regparam'])\n",
    "        max_alpha = float(p_ob[algo]['max_regparam'])\n",
    "\n",
    "        param_grid = param_grid = {'max_iter': range(min_iter, maxi_iter),\n",
    "\n",
    "                                   'alpha': [min_alpha, max_alpha]\n",
    "\n",
    "                                   }\n",
    "        grid_search = GridSearchCV(estimator=rr, param_grid=param_grid,\n",
    "                                   cv=3, n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        print(grid_search.best_params_)\n",
    "        best_grid = grid_search.best_estimator_\n",
    "        predi = best_grid.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, predi)\n",
    "        rmse = np.sqrt(mse)\n",
    "        return (rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e50e9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width      species\n",
      "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
      "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
      "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
      "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
      "4           5.0          3.6           1.4          0.2  Iris-setosa\n",
      "Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n",
      "       'species'],\n",
      "      dtype='object')\n",
      "dict_keys(['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
      "column_name:sepal_length\n",
      "the column sepal_length has 0 missing values\n",
      "In case of this variable we impute with,Average of values\n",
      "column_name:sepal_width\n",
      "the column sepal_width has 0 missing values\n",
      "In case of this variable we impute with,custom\n",
      "column_name:petal_length\n",
      "the column petal_length has 0 missing values\n",
      "In case of this variable we impute with,Average of values\n",
      "column_name:petal_width\n",
      "the column petal_width has 0 missing values\n",
      "In case of this variable we impute with,custom\n",
      "column_name:species\n",
      "the column species has 0 missing values\n",
      "Tokenize and hash\n",
      "Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 0, 1, 2,\n",
      "       3, 4],\n",
      "      dtype='object')\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n",
      "       'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5'],\n",
      "      dtype='object')\n",
      "4\n",
      "     sepal_length  sepal_width  petal_length  feature_2\n",
      "134           6.1          2.6           5.6   0.000000\n",
      "28            5.2          3.4           1.4  -1.000000\n",
      "65            6.7          3.1           4.4  -0.707107\n",
      "0             5.1          3.5           1.4  -1.000000\n",
      "27            5.2          3.5           1.5  -1.000000\n",
      "..            ...          ...           ...        ...\n",
      "75            6.6          3.0           4.4  -0.707107\n",
      "43            5.0          3.5           1.6  -1.000000\n",
      "22            4.6          3.6           1.0  -1.000000\n",
      "72            6.3          2.5           4.9  -0.707107\n",
      "15            5.7          4.4           1.5  -1.000000\n",
      "\n",
      "[105 rows x 4 columns]\n",
      "['RandomForestRegressor', 'GBTRegressor', 'LinearRegression', 'LogisticRegression', 'RidgeRegression', 'LassoRegression', 'ElasticNetRegression', 'DecisionTreeRegressor']\n",
      "RandomForestRegressor\n",
      "dict_keys(['model_name', 'is_selected', 'min_trees', 'max_trees', 'feature_sampling_statergy', 'min_depth', 'max_depth', 'min_samples_per_leaf_min_value', 'min_samples_per_leaf_max_value', 'parallelism'])\n",
      "{'bootstrap': False, 'max_depth': 22, 'min_samples_leaf': 5, 'n_estimators': 10}\n",
      "0.17024295396428477\n",
      "GBTRegressor\n",
      "dict_keys(['model_name', 'is_selected', 'num_of_BoostingStages', 'feature_sampling_statergy', 'use_deviance', 'use_exponential', 'fixed_number', 'min_subsample', 'max_subsample', 'min_stepsize', 'max_stepsize', 'min_iter', 'max_iter', 'min_depth', 'max_depth'])\n",
      "0.1998608061054044\n",
      "LinearRegression\n",
      "None\n",
      "LogisticRegression\n",
      "None\n",
      "RidgeRegression\n",
      "dict_keys(['model_name', 'is_selected', 'regularization_term', 'min_iter', 'max_iter', 'min_regparam', 'max_regparam'])\n",
      "{'alpha': 0.5, 'max_iter': 30}\n",
      "0.173659738999667\n",
      "LassoRegression\n",
      "dict_keys(['model_name', 'is_selected', 'regularization_term', 'min_iter', 'max_iter', 'min_regparam', 'max_regparam'])\n",
      "{'alpha': 0.5, 'max_iter': 30}\n",
      "0.35900760926317\n",
      "ElasticNetRegression\n",
      "dict_keys(['model_name', 'is_selected', 'regularization_term', 'min_iter', 'max_iter', 'min_regparam', 'max_regparam', 'min_elasticnet', 'max_elasticnet'])\n",
      "{'alpha': 0.5, 'l1_ratio': 0.5, 'max_iter': 30}\n",
      "0.28129573953486864\n",
      "DecisionTreeRegressor\n",
      "dict_keys(['model_name', 'is_selected', 'min_depth', 'max_depth', 'use_gini', 'use_entropy', 'min_samples_per_leaf', 'use_best', 'use_random'])\n",
      "{'max_depth': 7, 'min_samples_leaf': 6, 'splitter': 'random'}\n",
      "0.17974061145704634\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    path_data=\"E:/kartik/iris.csv\"\n",
    "\n",
    "    path_json=\"E:/kartik/Screening Test - DS/Screening Test - DS/algoparams_from_ui.json.rtf\"\n",
    "    data,p_ob,target=read_file(path_data,path_json)\n",
    "    feature_handling = p_ob['design_state_data']['feature_handling']\n",
    "    for i in data.columns:\n",
    "\n",
    "        tot_miss = data.loc[:, i].isna().sum() + data.loc[:, i].isnull().sum()\n",
    "        print(\"column_name:{}\".format(i))\n",
    "        print(\"the column {} has {} missing values\".format(i, tot_miss))\n",
    "\n",
    "        type = feature_handling[i]['feature_variable_type']\n",
    "        if type == 'numerical':\n",
    "            print(\"In case of this variable we impute with,\" + feature_handling[i]['feature_details']['impute_with'])\n",
    "            imp_with_val = feature_handling[i]['feature_details']['impute_with']\n",
    "            data = impute_num(imp_with_val, data, i)\n",
    "        else:\n",
    "            categories = data.loc[:, i].unique()\n",
    "            print(feature_handling[i]['feature_details']['text_handling'])\n",
    "            method = feature_handling[i]['feature_details']['text_handling']\n",
    "            n_data = impute_cat(method, data, i)\n",
    "            n_data = pd.DataFrame(n_data.toarray())\n",
    "            data = data.drop(columns=i)\n",
    "            data = pd.concat([data, n_data], axis=1)\n",
    "            print(data.columns)\n",
    "    # Replacing integer column names with string names\n",
    "    j = 1\n",
    "    for i in data.columns:\n",
    "        if i not in p_ob['design_state_data']['feature_handling'].keys():\n",
    "            print(\"here\")\n",
    "            data.rename({i: \"feature_\" + str(j)}, inplace=True, axis='columns')\n",
    "            j = j + 1\n",
    "    print(data.columns)\n",
    "    X_train,X_test,y_train,y_test=split_data(data,target)\n",
    "\n",
    "    # Extracting the parameters mentioned in the json file for feature reduction: tree based\n",
    "    f_way = p_ob['design_state_data']['feature_reduction']['feature_reduction_method']\n",
    "    features_selected=feature_reduction(f_way, X_train, y_train, p_ob['design_state_data']['feature_reduction'])\n",
    "    print(features_selected)\n",
    "    algo=p_ob['design_state_data']['algorithms'].keys()\n",
    "    selected_algo=[x for x in algo if 'regression' in x.lower() or 'regressor' in x.lower() ]\n",
    "    print(selected_algo)\n",
    "    for i in selected_algo:\n",
    "        #if p_ob['design_state_data']['algorithms'][i]['is_selected'] == True:\n",
    "            print(i)\n",
    "            rmse_m=model_build(i,X_train,y_train,p_ob['design_state_data']['algorithms'])\n",
    "            print(rmse_m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca1611f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
